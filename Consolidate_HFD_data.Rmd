---
title: "Consolidate HFD data"
author: "Alan Jackson"
date: '2022-05-07'
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(sf)

path <- "/home/ajackson/Dropbox/Rprojects/Curated_Data_Files/HFD_Incidents/"

googlecrs <- 4326

proj4string <-  "+proj=lcc +lat_1=28.38333333333333 +lat_2=30.28333333333333 +lat_0=27.83333333333333 +lon_0=-99 +x_0=600000 +y_0=3999999.999999999 +datum=NAD83 +units=us-ft +no_defs" # official current projection

#   EPSG:32615
proj4string_UTM <- "+proj=utm +zone=15 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
#   EPSG 26915
proj4string_UTM2 <- "+proj=utm +zone=15 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

proj4string_NAD27 <- "+proj=utm +zone=15 +ellps=clrk66 +units=m +no_defs "

#proj4string <- "+proj=utm +zone=15 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"

knitr::opts_chunk$set(echo = TRUE)
```

##    read in the extant files

Read in the files that have been downloaded so far

```{r read in}

filenames <- list.files(path = paste0(path, "Incrementals/"),
                        pattern="*_table.rds$")

filenames <- paste0(paste0(path, "Incrementals/"),filenames)

df <- filenames %>% 
  purrr::map_dfr(readRDS) %>% 
  unique() # get rid of duplicates


```







