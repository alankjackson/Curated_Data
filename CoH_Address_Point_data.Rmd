---
title: "Read in CoH Address POint data"
author: "Alan Jackson"
date: '2022-03-29'
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

path <- "/home/ajackson/Dropbox/Rprojects/Curated_Data_Files/CoH_Address_Points/"

filename <- "COH_PDD_ADDRESS_POINTS_-_PDD.csv"

df <- read_csv(paste0(path, filename),
               col_types="nnccccccccccccccnnccccccccccccccc")

```

## Quality control and Explore

Data read in mostly as character, except for lat long and xy
coordinates.

Let's explore each column, with an eye towards possibly transforming
them to a better data type (dates should be dates, for example)

```{r QC and Explore}
# Longitude

df %>% select(X) %>% 
  ggplot() +
  geom_histogram(aes(x=X))

# Latitude

df %>% select(Y) %>% 
  ggplot() +
  geom_histogram(aes(x=Y))

# Objectid

#   Can I safely convert to numeric?

df %>% select(OBJECTID) %>% 
  mutate(Objectid = as.integer(OBJECTID)) %>% 
  ggplot() +
  geom_histogram(aes(x=Objectid))

df %>% select(OBJECTID) %>% 
  mutate(Objectid = as.integer(OBJECTID)) %>% 
  count()
  
# ID (same question)

df %>% select(ID) %>% 
  mutate(Id = as.integer(ID)) %>% 
  ggplot() +
  geom_histogram(aes(x=Id))

# ADDR_ID (same question)

df %>% select(ADDR_ID) %>% 
  mutate(Addr_id = as.integer(ADDR_ID)) %>% 
  ggplot() +
  geom_histogram(aes(x=Addr_id))

# TYPES

df %>% select(TYPES) %>% 
  filter(!is.na(TYPES)) %>% 
  ggplot() +
  geom_histogram(aes(x=TYPES), stat="count") +
  coord_flip()

# STATUS

df %>% select(STATUS) %>% 
  ggplot() +
  geom_histogram(aes(x=STATUS), stat="count") +
  coord_flip()
```

Lat longs, and all the ID fields seem well enough behaved. Not sure
about TYPES, it seems to be left blank most of the time, so its
usefulness is questionable.

Status is curious. It is always filled in, and some of the meaning seem
obvious. ABD is obviously abandoned. CNTY = county, COH = City of
Houston. I think CP means that the location was reported? by Center
Point? I don't know what the rest mean. Not really a clue. Except that
UTA may have something to do with utilities.

```{r QC and Explore more}

# STREET_NUM

df %>% select(STREET_NUM) %>% 
  filter(str_starts(STREET_NUM, " ")) # test for beginning blank

df %>% select(STREET_NUM) %>% 
  filter(str_detect(STREET_NUM, " [0-9]")) # test for blank then number
  
dfnew <- df %>% 
  mutate(STREET_NUM = str_remove(STREET_NUM, "^[1-2]+/+[1-9]+")) %>% 
  mutate(STREET_NUM = str_remove(STREET_NUM, "[^0-9A-Z#/\\s]")) %>% 
  mutate(Street_alpha = str_trim(str_remove(STREET_NUM, "^\\d+")))  

dfnew %>% 
  filter(Street_alpha!="") %>% 
  ggplot() +
  geom_histogram(aes(x=Street_alpha), stat="count", na.rm=TRUE) +
  coord_flip() 
  
dfnew <- dfnew %>% 
  mutate(Street_num = as.numeric(str_extract(STREET_NUM,"^[0-9]+"))) %>% 
  filter(Street_num<49999)  

dfnew %>% 
  ggplot() +
  geom_histogram(aes(x=Street_num)) 
  

```

### STREET_NUM

Now it gets messy.

The street numbers have junk in them - sometimes.

4107 B 606 #3 1/2 35 1/2 13410.

So we'll try to clean up at least most of this. We'll create two new
variables, Street_num and Street_alpha.

```{r QC and explore even more}

#   Fraction

dfnew %>% 
  filter(FRACTION!="") %>% 
  select(FRACTION) %>% 
  unique

```

Hmm... some trash in the FRACTION field. Period, square bracket,
slash... Sometimes it matches what I have designated Street_alpha, but
not always.

```{r PREFIX}

dfnew %>% 
  filter(PREFIX!="") %>% 
  select(PREFIX) %>% 
  unique

dfnew <- 
dfnew %>% 
  mutate(PREFIX=str_to_upper(PREFIX))

dfnew$PREFIX[!is.na(dfnew$PREFIX) & str_detect(dfnew$PREFIX, "[^NSEW]")] <- NA

```

A few bad entries. the "C" is pulled from the street number so can be
deleted. The lower case directions can be repaired.

```{r STREET NAME}

euclidean <- function(a, b) sqrt(sum((a - b)^2))

dfnew %>% count(STREET_NAME, sort=TRUE) %>% 
  filter(n<5)

dfnew %>% count(STREET_NAME, sort=TRUE) %>% 
  filter(n<10) %>% 
  ggplot() +
  geom_histogram(aes(x=n))

#   We have some bad names. Let's look at low occurrence names and have nearby 
#   names vote on the proper name

#   First shrink dataset size, then make an sf object
#   Nah, just use zipcode and text distance
#   But then look at physical distance as a fine tuning check

#   State Plane, Texas South Central Zone 5401, FIPS 4204, NAD83, EPSG 2278

dftest <- 
dfnew %>% 
#  head() %>% 
  #select(OBJECTID, STREET_NAME, Street_num, ZIPCODE)# %>% 
  select(OBJECTID, STREET_NAME, Street_num, ZIPCODE, X_COORD, Y_COORD)# %>% 
  #sf::st_as_sf(., coords=c("X_COORD", "Y_COORD"))

#   data frame of single occurrence names
df_singles <- dftest %>% 
  group_by(STREET_NAME) %>% 
  mutate(count=n()) %>% 
  filter(count<2) %>% 
  select(-count) %>% 
  filter(str_length(STREET_NAME)>3) %>% #  eliminate short names
  filter(!str_detect(STREET_NAME, "STREET")) %>% # eliminate A STREET, B STREET
  filter(!str_detect(STREET_NAME, "^\\d+TH")) %>% # eliminate 4TH, 5TH, etc
  filter(!str_detect(STREET_NAME, "^CR \\d+")) # eliminate county roads

#   data frame of double occurrence names
df_doubles <- dftest %>% 
  group_by(STREET_NAME) %>% 
  mutate(count=n()) %>% 
  filter(count==2) %>% 
  select(-count) %>% 
  filter(str_length(STREET_NAME)>3) %>% #  eliminate short names
  filter(!str_detect(STREET_NAME, "STREET")) %>% # eliminate A STREET, B STREET
  filter(!str_detect(STREET_NAME, "^\\d+TH")) %>% # eliminate 4TH, 5TH, etc
  filter(!str_detect(STREET_NAME, "^CR \\d+")) # eliminate county roads

##    function to get text distances and geometric distances

get_corrections <- function(df_test, df_base, text_dist, geom_dist, min_votes){
  tmp <- NULL
  for (i in 1:nrow(df_test)) {
    foo <- df_test[i,]
    foobar <- 
    df_base %>% filter(ZIPCODE==foo$ZIPCODE) %>% 
      mutate(adist=adist(STREET_NAME, foo$STREET_NAME),
             bad_X=foo$X_COORD,
             bad_Y=foo$Y_COORD,
             bad_num=foo$Street_num,
             bad_name=foo$STREET_NAME,
             bad_id=foo$OBJECTID) %>% 
      filter(adist == text_dist) %>% 
      mutate(eucdist=sqrt((X_COORD - bad_X)^2 +
                          (Y_COORD-bad_Y)^2)) %>% 
      group_by(STREET_NAME) %>%
         summarize(n=n(),
                   Street_num=last(bad_num),
                   STREET_NAME=last(STREET_NAME),
                   bad_name=last(bad_name),
                   bad_id=last(bad_id),
                   min_dist=min(eucdist)
                   ) %>%
      ungroup() %>%
      filter(n>min_votes,
             min_dist<geom_dist)
    
      tmp <- bind_rows(tmp, foobar) 
  }
  return(tmp)
}

#  Loop through singles, get distances to all in zipcode, then get text distance

tmp <- get_corrections(df_singles, dftest, 1, 1060, 3)

df_singles <- df_singles[!df_singles$OBJECTID %in% tmp$bad_id,]
tmp[duplicated(tmp$bad_id),]
tmp <- tmp %>% filter(!STREET_NAME=="INTERCONTINENT",
                      !STREET_NAME=="MARY")

tmp2 <- get_corrections(df_singles, dftest, 2, 300, 5)

df_singles <- df_singles[!df_singles$OBJECTID %in% tmp2$bad_id,]
df_singles <- df_singles %>% filter(!OBJECTID=="76158") %>% 
                             filter(!OBJECTID=="705381")
tmp2[duplicated(tmp2$bad_id),]

tmp3 <- get_corrections(df_singles, dftest, 3, 300, 9)
tmp3[duplicated(tmp3$bad_id),]
    
#------   Now do doubles

tmp4 <- get_corrections(df_doubles, dftest, 1, 500, 8)
tmp4[duplicated(tmp4$bad_id),]

#------ Now update the original by adding a new field

dfnew <- dfnew %>% mutate(New_street=STREET_NAME)

dfnew[dfnew$OBJECTID %in% tmp$bad_id,]$New_street <- tmp$STREET_NAME 
dfnew[dfnew$OBJECTID %in% tmp2$bad_id,]$New_street <- tmp2$STREET_NAME 
dfnew[dfnew$OBJECTID %in% tmp3$bad_id,]$New_street <- tmp3$STREET_NAME 
dfnew[dfnew$OBJECTID %in% tmp4$bad_id,]$New_street <- tmp4$STREET_NAME 

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_1"))

```

```{r reload}

dfnew <- readRDS(paste0(path, "temporary_intermediate_file_1"))

```

```{r suffixes}

dfnew %>% 
  filter(SUFFIX!="") %>% 
  select(SUFFIX) %>% 
  unique

dfnew$SUFFIX[!is.na(dfnew$SUFFIX) & str_detect(dfnew$SUFFIX, "[^NSEW]")] <- NA

```

```{r street type}

dfnew %>% 
  filter(STREET_TYPE!="") %>% 
  select(STREET_TYPE) %>% 
  unique

Street_types <- 
dfnew %>% 
      group_by(STREET_TYPE) %>%
         summarize(n=n(),
                   STREET_TYPE=last(STREET_TYPE)) %>% 
  arrange(-n)

#   a lot of mess here. Will use a similar strategy as used for street names.
#   look at nearby friends and try to grab the type from them. Use total dataset
#   n value to establish the "standard".
#   Two tasks - look for missing types by checking nearby friends
#   And check the existing type against the standards

Allowed_types <- c(
  "ALY", "ANX", "ARC", "AVE", "BYU", "BCH", "BND", "BLF", "BLFS", "BTM", "BLVD",
  "BR", "BRG", "BRK", "BRKS", "BG", "BGS", "BYP", "CP", "CYN", "CPE", "CSWY", "CTR",
  "CTRS", "CIR", "CIRS", "CLF", "CLFS", "CLB", "CMN", "CMNS", "COR", "CORS",
  "CRSE", "CT", "CTS", "CV", "CVS", "CRK", "CRES", "CRST", "XING", "XRD", "XRDS",
  "CURV", "DL", "DM", "DV", "DR", "DRS", "EST", "ESTS", "EXPY", "EXT", "EXTS", "FALL",
  "FLS", "FRY", "FLD", "FLDS", "FLT", "FLTS", "FRD", "FRDS", "FRST", "FRG", "FRGS",
  "FRK", "FRKS", "FT", "FWY", "GDN", "GDNS", "GTWY", "GLN", "GLNS", "GRN", "GRNS",
  "GRV", "GRVS", "HBR", "HBRS", "HVN", "HTS", "HWY", "HL", "HLS", "HOLW", "INLT",
  "IS", "ISS", "ISLE", "JCT", "JCTS", "KY", "KYS", "KNL", "KNLS", "LK", "LKS",
  "LAND", "LNDG", "LN", "LGT", "LGTS", "LF", "LCK", "LCKS", "LDG", "LOOP", "MALL",
  "MNR", "MNRS", "MDW", "MDWS", "MEWS", "ML", "MLS", "MSN", "MTWY", "MT", "MTN",
  "MTNS", "NCK", "ORCH", "OVAL", "OPAS", "PARK", "PKWY", "PASS", "PSGE", "PATH",
  "PIKE", "PNE", "PNES", "PL", "PLN", "PLNS", "PLZ", "PT", "PTS", "PRT", "PRTS",
  "PR", "RADL", "RAMP", "RNCH", "RPD", "RPDS", "RST", "RDG", "RDGS", "RIV", "RD",
  "RDS", "RTE", "ROW", "RUE", "RUN", "SHL", "SHLS", "SHR", "SHRS", "SKWY", "SPG",
  "SPGS", "SPUR", "SQ", "SQS", "STA", "STRA", "STRM", "ST", "STS", "SMT", "TER",
  "TRWY", "TRCE", "TRAK", "TRFY", "TRL", "TRLR", "TUNL", "TPKE", "UPAS", "UN",
  "UNS", "VLY", "VLYS", "VIA", "VW", "VWS", "VLG", "VLGS", "VL", "VIS", "WALK",
  "WALL", "WAY", "WAYS", "WL", "WLS")


#   Let's look for erroneous types

Street_types %>% filter(!is.na(STREET_TYPE)) %>% 
  filter(!STREET_TYPE %in% Allowed_types)

#   Well that is simple enough. In Houston, apparently, "Speedway" is a street
#   type, so I'll leave that one alone. The rest I can update to the correct
#   value.

dfnew <- dfnew %>% 
  mutate(STREET_TYPE=str_replace(STREET_TYPE, "^PKY$", "PKWY")) %>% 
  mutate(STREET_TYPE=str_replace(STREET_TYPE, "^ROAD$", "RD")) %>% 
  mutate(STREET_TYPE=str_replace(STREET_TYPE, "^LANE$", "LN")) %>% 
  mutate(STREET_TYPE=str_replace(STREET_TYPE, "^TWY$", "CT"))  # not a tollway
  
#   Look at the NA's to see if they make sense

#   For each street/zip pair sum up # types

tmp <- 
dfnew %>% group_by(ZIPCODE, STREET_NAME) %>% 
      summarise(n=n(),
                sumNA = sum(is.na(STREET_TYPE)),
                NotNAs=list(unique(STREET_TYPE[!is.na(STREET_TYPE)]))) %>% 
  filter(!sumNA==n) %>% 
  mutate(NotNA=n-sumNA) %>% 
  mutate(Flag=lengths(NotNAs)) %>% 
  filter(sumNA>0) %>% 
  mutate(NotNAs=paste(NotNAs)) %>% # convert list to string 
  filter(Flag==1)  # remove ambiguous cases where there are multiple types

#   Let's first fix things that are not NA that should be
#   

#   Add new field so we retain original values

dfnew <- dfnew %>% mutate(Street_type = STREET_TYPE) 

tmp_rm_not_na <- tmp %>% 
  filter(n>9) %>% # need at least 10 to believe statistics
  mutate(foo=(NotNA/n)) %>% 
  filter((NotNA/n)<0.151) %>% 
  filter(!NotNAs=="CT") # don't trust 'court', leave alone

#   Apply what is left to NA that field

for (i in 1:nrow(tmp_rm_not_na)){
  dfnew$Street_type[(dfnew$ZIPCODE==tmp_rm_not_na[[i,1]]) & 
          (dfnew$STREET_NAME==tmp_rm_not_na[[i,2]])] <- NA
}
  
# Now let's fix things that are NA but shouldn't be

tmp_rm_na <- tmp %>% 
  filter(n>9) %>% # need at least 10 to believe statistics
  mutate(foo=(NotNA/n)) %>% 
  filter((NotNA/n)>0.810) %>% 
  filter(!NotNAs=="CT") %>% # don't trust 'court', leave alone
  filter(!NotNAs=="CIR") %>% # don't trust 'circle', leave alone
  filter(!NotNAs=="PL") # don't trust 'place', leave alone

#   Apply what is left to fill that field

for (i in 1:nrow(tmp_rm_na)){
  dfnew$Street_type[(dfnew$ZIPCODE==tmp_rm_na[[i,1]]) & 
          (dfnew$STREET_NAME==tmp_rm_na[[i,2]])] <- tmp_rm_na[[i,5]]
}
  
saveRDS(dfnew, paste0(path, "temporary_intermediate_file_2"))


```

```{r revisit prefix}

#   Can we grab nearby prefixes to repair missing ones?

#   Let's shrink dataset size
dftest <- 
dfnew %>% 
  select(OBJECTID, Street_num, Street_alpha, PREFIX, New_street, 
         Street_type, ZIPCODE, X_COORD, Y_COORD) 

#   Now start analysis by street name, type, and zipcode

tmp <- 
dftest %>% group_by(ZIPCODE, New_street, Street_type) %>% 
      summarise(n=n(),
                sumNA = sum(is.na(PREFIX)),
                sumE = sum(PREFIX=="E", na.rm=TRUE),
                sumW = sum(PREFIX=="W", na.rm=TRUE),
                sumN = sum(PREFIX=="N", na.rm=TRUE),
                sumS = sum(PREFIX=="S", na.rm=TRUE)) %>%
  mutate(singlet = n-sumNA) %>% 
  filter(!sumNA==n) %>% 
  filter(sumNA>0) %>% 
  mutate(percentNA = sumNA/n) 

#   Take singlet prefixes with total n>10 and find minimum distance to
#   a cousin

#   data frame of single occurrence PREFIXes
df_singles <- tmp %>% 
  filter(n>10) %>% 
  filter(singlet==1) %>% 
  filter(!Street_type == "CIR") %>% # eliminate circles
  filter(!Street_type == "CT") %>%   # eliminate courts
  mutate(nearest=NA)

for (i in 1:nrow(df_singles)) {
  tmp_sing <- dftest %>% filter((ZIPCODE==df_singles[i,]$ZIPCODE) &
                                (New_street==df_singles[i,]$New_street) & 
                                (Street_type==df_singles[i,]$Street_type))
  
  bad_X <- tmp_sing[!is.na(tmp_sing$PREFIX),]$X_COORD
  bad_Y <- tmp_sing[!is.na(tmp_sing$PREFIX),]$Y_COORD
  
  tmp_sing <- tmp_sing %>% 
    mutate(dist=sqrt((X_COORD - bad_X)^2 +
                     (Y_COORD-bad_Y)^2)) %>% 
    arrange(dist)
  
  df_singles[i,]$nearest <- tmp_sing[2,]$dist
}

#   Let's use a 150 foot cutoff. Further away than that and I'll ignore it.

df_singles <- df_singles %>% filter(nearest<150)

dfnew <- dfnew %>% mutate(Prefix=PREFIX)
  
dfnew[(dfnew$New_street %in% df_singles$New_street) &
  (dfnew$ZIPCODE %in% df_singles$ZIPCODE) &
  (dfnew$Street_type %in% df_singles$Street_type),]$Prefix <- NA 

#######   now let's look at NA's that shouldn't be.
#   This is more complicated since the needed prefix could be E or W say.
#   About 0.1 for the percent cutoff seems reasonable

#   data frame of candidates
df_cands <- tmp %>% 
  filter(n>10) %>% 
  filter(percentNA<0.1) %>% 
  filter(!Street_type == "CIR") %>% # eliminate circles
  filter(!Street_type == "CT")  # eliminate courts

#   Expand this out to all the individual points in prep for distance calc
#   Let's shrink dataset size
dftest <- 
dfnew %>% 
  select(OBJECTID, Street_num, Street_alpha, Prefix, New_street, 
         Street_type, ZIPCODE, X_COORD, Y_COORD) 
dftest$Nearest <- 0
dftest$Minimus <- 0
dftest$Neighbor_object <- NA

for (i in 1:nrow(df_cands)){
#for (i in 1:3){
  print(paste(i, "---->", df_cands[i,]$New_street))
  eraseme <- dftest %>% 
    filter((ZIPCODE %in% df_cands[i,]$ZIPCODE) &
           (New_street %in% df_cands[i,]$New_street) &
           (Street_type %in% df_cands[i,]$Street_type))  
  
  targets <- eraseme %>% filter(is.na(Prefix)) %>% 
    mutate(Prefix=NA,
           dist=0)
  
  for (j in 1:nrow(targets)){
    foo <-   eraseme %>% 
    summarize(nearest=sqrt((X_COORD -targets[j,]$X_COORD)^2 +
                        (Y_COORD-targets[j,]$Y_COORD)^2),
              prefix=Prefix,
              object=OBJECTID) %>% 
      arrange(nearest) %>% 
      drop_na()
    
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Prefix <- foo[1,]$prefix
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Nearest <- foo[1,]$nearest
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Neighbor_object <- foo[1,]$object
    print("--b--")
  }
}

dftest %>% filter(Nearest>0,
                 Nearest<300) %>% 
  ggplot() +
  geom_histogram(aes(x=Nearest))

#   150 ft feels like the furthest I can do with confidence.

foo <- dftest %>% filter(Nearest<150,
                         Nearest>0)
                   
dfnew[dfnew$OBJECTID %in% foo$OBJECTID,]$Prefix <- foo$Prefix

```

##  Updating prefix

For a few points the zipcode is incorrect. For example, 4436 MacGregor is shown
as in 77004, but actually is in 77021. This ends up giving me an unreasonable
nearest point.

```{r City}

dfnew$CITY %>% unique() %>% sort()


```

City names look okay.

```{r zips}

dfnew$ZIPCODE %>% unique() %>% sort()

```

Zipcodes look okay.

```{r sources}

dfnew$SOURCES %>% unique() %>% sort()


```

A little ambiguity between "COH" and "City of Houston", also CP and Centerpoint
Energy, Center Point, and CE.
```{r last edit date}

dfnew %>% mutate(date=lubridate::ymd_hms(last_edited_date)) %>% 
  ggplot() +
  geom_histogram(aes(x=date))

```

No last edited dates before 2015.
