---
title: "Read in CoH Address gdb data"
author: "Alan Jackson"
date: '2024-09-27'
format: html
description: "Create a new file suitable for geocoding from the City of Houston address point data"
image: "cover.png"
categories:
  - Mapping
  - Data
execute:
  freeze: auto  # re-render only when source changes
  warning: false
editor: source
---

##    Read a clean COH address data

Data downloaded on Sept 27, 2024 from

https://mycity.maps.arcgis.com/home/item.html?id=aeb4bb6b3b25471caf4d883bf979424d

which can be found from the page

https://cohgis-mycity.opendata.arcgis.com/

We will go through the data, looking for errors and correcting them when
possible. We will also produce a separate file of those corrections, so there
will be an audit trail of what was done.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

path <- "/home/ajackson/Dropbox/Rprojects/Curated_Data_Files/CoH_Address_Points/"

googlecrs <- "EPSG:4326"
COH_crs <- "EPSG:2278"

filename <- "SITEADDRESSES_20240808.gdb"

layers <- sf::st_layers(paste0(path, filename))

df <- sf::st_read(paste0(path, filename), 
                    layer = "SiteAddresses")

corrections <- NULL # collect what has been changed


#   Some fields are all NA. I'll go ahead and remove those now
#     unitrange to and from have only about 20 non-NA values. Also
#     altunitid has 3 non NA's
#   Validationstatus is always = 2

df <- df %>% 
  select(-esn, -propertytaxid, -unitrangeto, -unitrangefrom, -altunitid,
         -altunittype, -secondaltunitid, -secondaltunittype, -addrrange,
         -pointtype, -openlocation, -capturemeth, -msag, -VALIDATIONSTATUS)

```

## Quality control and Explore

Data read in mostly as character, except for lat long and xy
coordinates.

Let's explore each column, with an eye towards possibly transforming
them to a better data type (dates should be dates, for example)

```{r QC and Explore}

# Longitude

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(xcoord) %>% 
  ggplot() +
  geom_histogram(aes(x=xcoord))

# Latitude

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(ycoord) %>% 
  ggplot() +
  geom_histogram(aes(x=ycoord))

# siteaddid

#   Can I safely convert to numeric?

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  mutate(siteaddid = as.integer(siteaddid)) %>% 
  ggplot() +
  geom_histogram(aes(x=siteaddid))

#   Is siteaddid a unique key? (spoiler alert - yes)

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  mutate(siteaddid = as.integer(siteaddid)) %>% 
  unique() %>% 
  count()

#   Is siteaddid_old a unique key? (spoiler alert - yes)

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(siteaddid_old) %>% 
  mutate(siteaddid_old = as.integer(siteaddid_old)) %>% 
  unique() %>% 
  count()

# ADDR_ID (unique? Nope)

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addressptid) %>% 
  mutate(addressptid = as.integer(addressptid)) %>% 
  ggplot() +
  geom_histogram(aes(x=addressptid))

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addressptid) %>% 
  mutate(addressptid = as.integer(addressptid)) %>% 
  unique() %>% 
  count()

# v911 (0, 1, or NA)

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(v911) %>% 
  mutate(v911 = as.integer(v911)) %>% 
  ggplot() +
  geom_histogram(aes(x=v911))

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(v911) %>% 
  mutate(v911 = as.integer(v911)) %>% 
  unique() %>% 
  count()

# addrtype

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addrtype) %>% 
  filter(!is.na(addrtype)) %>% 
  ggplot() +
  geom_histogram(aes(x=addrtype), stat="count") +
  coord_flip()

# source

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(source) %>% 
  filter(!is.na(source)) %>% 
  ggplot() +
  geom_histogram(aes(x=source), stat="count") +
  coord_flip()

# STATUS

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(status) %>% 
  ggplot() +
  geom_histogram(aes(x=status), stat="count") +
  coord_flip()

# municipality

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(municipality) %>% 
  ggplot() +
  geom_histogram(aes(x=municipality), stat="count") +
  coord_flip()

```

Most fields seem well-behaved. However, "addrtype" looks like it contains old
database data as well as new database data, and will need cleanup. For example,
ABD is almost certainly the same as "Abandon".

Similarly, there are clearly similar issues with "source", which will need 
some cleanup.

Lat longs, and all the ID fields seem well enough behaved. 

Status is curious. It is always filled in, and some of the meaning seem obvious.
CNTY = county, COH = City of Houston. 
CP are weird. Some are parking meters, fire hydrants, utility boxes. etc.
UTA also seem to be utilities - cell tower, bus stop, electric pole,
parking meter, etc.
OOR means "Out Of Range". Which I think means that the addresses are not
monotonic, but jump around - which is bad. I will have to keep those but use
the flag to not use them to interpolate.

##        Do some cleanup

```{r first cleanup}

#   source field redundancies

df <- df %>% 
  mutate(source=str_replace(source, "MONT", "Montgomery 911")) %>% 
  mutate(source=str_replace(source, "RVSG", "Reverse Geocode")) %>% 
  mutate(source=str_replace(source, "GHC911", "Greater Harris County 911")) %>% 
  mutate(source=str_replace(source, "CP", "Center Point")) %>% 
  mutate(source=str_replace(source, "CE", "Code Enforcement")) %>% 
  mutate(source=str_replace(source, "COH", "City of Houston")) %>% 
  mutate(source=str_replace(source, "HARRIS", "Harris Central Appraisal District"))  

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(source) %>% 
  filter(!is.na(source)) %>% 
  ggplot() +
  geom_histogram(aes(x=source), stat="count") +
  coord_flip()

#   addrtype redundancies

df <- df %>% 
  mutate(addrtype=str_replace(addrtype, "VAC", "Vacant")) %>% 
  mutate(addrtype=str_replace(addrtype, "UNK", "Unknown")) %>% 
  mutate(addrtype=str_replace(addrtype, "RESERVE", "Reserve")) %>% 
  mutate(addrtype=str_replace(addrtype, "PARCEL", "Parcel")) %>% 
  mutate(addrtype=str_replace(addrtype, "NonStr", "Non Structural")) %>% 
  mutate(addrtype=str_replace(addrtype, "INDUSTRY", "Industry")) %>% 
  mutate(addrtype=str_replace(addrtype, "COMMERCIAL", "Commercial")) %>% 
  mutate(addrtype=str_replace(addrtype, "ABD", "Abandon")) %>% 
  mutate(addrtype=str_replace(addrtype, "PubPark", "Public Park")) %>% 
  mutate(addrtype=str_replace(addrtype, "RES", "Residential")) 

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addrtype) %>% 
  filter(!is.na(addrtype)) %>% 
  ggplot() +
  geom_histogram(aes(x=addrtype), stat="count") +
  coord_flip()

```

```{r QC and Explore more}

############
# addrnum
############

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addrnum) %>% 
  filter(str_starts(addrnum, " ")) # test for beginning blank

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(addrnum) %>% 
  filter(str_detect(addrnum, " [0-9]")) # test for blank then number
  
    #   remove beginning blanks and beginning zeros
dfnew <- df %>% 
  mutate(addrnum = str_remove(addrnum, "^ ")) %>% 
  mutate(addrnum = str_remove(addrnum, "^0"))  

corrections <- df %>% 
  filter(str_detect(addrnum, "^ ") |
         str_detect(addrnum, "^0")) %>%  
  mutate(change="Extraneous stuff in addrnum")

#   Drop weird, bogus addresses (99999 OLD KATY RD, 99999 OFF CARSON, 
#   99999 OFF WEBERCREST)

dfnew <- dfnew %>% 
  filter(siteaddid!=309237) %>% 
  filter(siteaddid!=468637) %>% 
  filter(siteaddid!=1184918)  

############
# fraction
############

df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  select(fraction) %>% 
  filter(!is.na(fraction)) %>% 
  ggplot() +
  geom_histogram(aes(x=fraction), stat="count") 

#   get rid of 1/1 fraction and convert blank to NA (which is faster)

dfnew <- dfnew %>% 
  mutate(fraction = replace(fraction, which(
                            str_detect(fraction, "^1/1$|^\\s*$")), NA))

tmp <-   df %>% 
  filter((siteaddid == 309237) |
         (siteaddid == 468637) |
         (siteaddid == 1184918)) %>% 
  mutate(change="Bogus addresses")  

corrections <- bind_rows(corrections, tmp)

#   Let's look at addresses larger than 50,000. Are all of these problematic?

tmp <- df %>% sf::st_drop_geometry() %>% # This will speed things considerably
  mutate(addrnum=as.integer(addrnum)) %>% 
  filter(addrnum>50000)  

#   These need corrections, the rest, delete.
From_to <- tribble(~fromaddrnum, ~fromstreet, ~toaddrnum,
"144407", "BROWNSVILLE ST", "14407",
"187472", "COMANCHE DR", "18472",
"68260", "SATSUMA", "6826",
"61200", "CENTER", "612",
"56120", "CHAUCER", "5612",
"96615", "STAG LN", "26615",
"191116", "SUFFOLK", "19116",
"83202", "TERRA VALLEY LN", "8302",
"80300", "INDIANA", "803",
"93077", "SUMMERBELL LN", "9307",
"296937", "LEGENDS GREEN DR", "29637",
"191232", "RYE CT", "19132",
"71201", "KANSAS", "712",
"63010", "DE PRIEST ST", "6310",
"165018", "AVENPLACE RD", "16518",
"79075", "WOODWAY DR", "7905",
"183151", "KUYKENDAHL RD", "18315",
"105415", "S SUNRISE SHORES LN", "10415",
"246219", "BALLARD DR", "24621",
"167007", "MULBEN CT", "17007",
"168006", "HIGHLAND COUNTRY DR", "16806",
"102115", "DAWSON HILL LN", "10215",
"153155", "HOLLOWAY HILLS TRL", "15315",
"82011", "HAWTHORNE VALLEY LN", "8211",
"188057", "BRESCIA LN", "18807",
"189011", "BRESCIA LN", "19011",
"114510", "JONSTONE PAISLEY CT", "11510",
"220122", "GRAND MIST DR", "22022",
"72314", "CYPRESS SHUMARD OAK DR", "7314",
"243327", "MARCELLO LAKES DR", "24327",
"153042", "ROSEHILL SUMMIT LN", "15302",
"248179", "PUCCINI PL", "24819",
"61002", "HACKBERRY BRANCH LN", "6102",
"115114", "KIRKSHAW DR", "11514",
"61606", "BLUE SAPPHIRE CT", "21606",
"54311", "ALTO GARDEN DR", "24311",
"50274", "BROOM ST", "5024",
"226938", "DRYBANK CREEK LN", "26938",
"113131", "HILLSIDE KNOLL LN", "11313",
"199252", "PURUS DR", "19252",
"2525718", "ROYAL CATCHFLY RD", "25718",
"197702", "HICKORY HEIGHTS DR", "19702"
) %>%
  mutate(fulladdr=paste(fromaddrnum, fromstreet))

dfnew <- dfnew %>% 
  left_join(., From_to, by="fulladdr") %>% 
  mutate(addrnum=if_else(is.na(toaddrnum), addrnum, toaddrnum)) %>% 
  select(-fromaddrnum, -fromstreet, -toaddrnum)

tmp <- df %>% 
  left_join(., From_to, by="fulladdr") %>% 
  filter(! is.na(toaddrnum)) %>% 
  mutate(addrnum=toaddrnum) %>% 
  select(-fromaddrnum, -fromstreet, -toaddrnum) %>% 
  mutate(change="Repair bad addresses")  

corrections <- bind_rows(corrections, tmp)

#   Drop some hopeless addresses

tmp <- dfnew %>% 
  mutate(addrtest=as.integer(addrnum)) %>% 
  filter(addrtest>=50000) %>% 
  select(-addrtest) %>% 
  mutate(change="delete records with irrepairable or duplicate addresses")  

corrections <- bind_rows(corrections, tmp)
  
dfnew <- dfnew %>% 
  mutate(addrtest=as.integer(addrnum)) %>% 
  filter(addrtest<50000) %>% 
  select(-addrtest)
  
```

##    Let's check out the fraction field

```{r fraction}

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  filter(!is.na(fraction)) %>% 
  select(fraction) %>% 
  unique()

```

Looks like the fractions are well-behaved.

##        Prefix

How nicely behaved are the prefixes?

```{r prefix}

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  filter(!is.na(prefix)) %>% 
  select(prefix) %>% 
  unique

```

No prefix issues. Awesome!


##        Street Names

Look at low occurrence names. 
- names that occur only once
  - update names that are < 1060 ft from a 1 char change in 3 names in the zip
  - then 2 character changes < 300 feet with >= 5 names in zip
  - then 3 character changes < 300 feet with >= 9 names in zip
- names that occur twice
  - update names that are < 500 ft from a 1 char change in 8 names in the zip

```{r STREET NAME}

euclidean <- function(a, b) sqrt(sum((a - b)^2))

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  count(roadname, sort=TRUE) %>% 
  filter(n<5)

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  count(roadname, sort=TRUE) %>% 
  filter(n<10) %>% 
  ggplot() +
  geom_histogram(aes(x=n))

#   We have some bad names. Let's look at low occurrence names and have nearby 
#   names vote on the proper name

#   First shrink dataset size, then make an sf object
#   Nah, just use zipcode and text distance
#   But then look at physical distance as a fine tuning check

#   State Plane, Texas South Central Zone 5401, FIPS 4204, NAD83, EPSG 2278

dftest <- 
dfnew %>% 
  # head() %>% 
  select(siteaddid, roadname, addrnum, zipcode, xcoord, ycoord) %>% 
  sf::st_drop_geometry()
  #sf::st_as_sf(., coords=c("X_COORD", "Y_COORD"))

#   data frame of single occurrence names
df_singles <- dftest %>% 
  group_by(roadname) %>% 
  mutate(count=n()) %>% 
  filter(count<2) %>% 
  select(-count) %>% 
  filter(str_length(roadname)>3) %>% #  eliminate short names
  filter(!str_detect(roadname, "STREET")) %>% # eliminate A STREET, B STREET
  filter(!str_detect(roadname, "^\\d+TH")) %>% # eliminate 4TH, 5TH, etc
  filter(!str_detect(roadname, "^CR \\d+")) # eliminate county roads

#   data frame of double occurrence names
df_doubles <- dftest %>% 
  group_by(roadname) %>% 
  mutate(count=n()) %>% 
  filter(count==2) %>% 
  select(-count) %>% 
  filter(str_length(roadname)>3) %>% #  eliminate short names
  filter(!str_detect(roadname, "STREET")) %>% # eliminate A STREET, B STREET
  filter(!str_detect(roadname, "^\\d+TH")) %>% # eliminate 4TH, 5TH, etc
  filter(!str_detect(roadname, "^CR \\d+")) # eliminate county roads

##    function to get text distances and geometric distances

get_corrections <- function(df_test, df_base, text_dist, geom_dist, min_votes){
  tmp <- NULL
  for (i in 1:nrow(df_test)) {
    foo <- df_test[i,]
    foobar <- 
    df_base %>% filter(zipcode==foo$zipcode) %>% 
      mutate(adist=adist(roadname, foo$roadname),
             bad_X=foo$xcoord,
             bad_Y=foo$ycoord,
             bad_num=foo$addrnum,
             bad_name=foo$roadname,
             bad_id=foo$siteaddid) %>% 
      filter(adist == text_dist) %>% 
      mutate(eucdist=sqrt((xcoord - bad_X)^2 +
                          (ycoord-bad_Y)^2)) %>% 
      group_by(roadname) %>%
         summarize(n=n(),
                   Street_num=last(bad_num),
                   STREET_NAME=last(roadname),
                   bad_name=last(bad_name),
                   bad_id=last(bad_id),
                   min_dist=suppressWarnings(min(eucdist))
                   ) %>%
      ungroup() %>%
      filter(n>min_votes,
             min_dist<geom_dist)
    
      tmp <- bind_rows(tmp, foobar) 
  }
  return(tmp)
}

#  Loop through singles, get distances to all in zipcode, then get text distance

tmp <- get_corrections(df_singles, dftest, 1, 1060, 3)

#   What's left that we didn't find a correction for?
df_singles <- df_singles[!df_singles$siteaddid %in% tmp$bad_id,]
#     Get rid of duplicates
tmp[duplicated(tmp$bad_id),]
#     These are special and need special work
tmp <- tmp %>% filter(!STREET_NAME=="INTERCONTINENTAL",
                      !STREET_NAME=="MARY")

dfnew[dfnew$siteaddid=="619144",]$addrnum <- "1989"
dfnew <- dfnew %>% 
  mutate(roadname=str_replace(roadname, "INTERCONTINENT$|INTERCONTINENTA$", "INTERCONTINENTAL"))

foo <- df[df$siteaddid %in% tmp$bad_id,] %>% 
  mutate(change=paste("Corrected STREET_NAME to",
                      tmp[siteaddid %in% tmp$bad_id,]$STREET_NAME))
#   Add corrections for singles
corrections <- bind_rows(corrections, foo)

#   Singles with 2 letter changes
tmp2 <- get_corrections(df_singles, dftest, 2, 300, 5)

#   What's left that we didn't find a correction for?
df_singles <- df_singles[!df_singles$siteaddid %in% tmp2$bad_id,]
# df_singles <- df_singles %>% filter(!siteaddid=="76158") %>% 
                             # filter(!siteaddid=="705381")
#     Get rid of duplicates
tmp2[duplicated(tmp2$bad_id),]

foo <- df[df$siteaddid %in% tmp2$bad_id,] %>% 
  mutate(change=paste("Corrected STREET_NAME to",
                      tmp2[siteaddid %in% tmp2$bad_id,]$STREET_NAME))
#   Add corrections for singles
corrections <- bind_rows(corrections, foo)

#   Singles with 3 letter changes
tmp3 <- get_corrections(df_singles, dftest, 3, 300, 9)
tmp3[duplicated(tmp3$bad_id),]

foo <- df[df$siteaddid %in% tmp3$bad_id,] %>% 
  mutate(change=paste("Corrected STREET_NAME to",
                      tmp3[siteaddid %in% tmp3$bad_id,]$STREET_NAME))
#   Add corrections for singles
corrections <- bind_rows(corrections, foo)
    
#------   Now do doubles

df_doubles <- df_doubles %>% ungroup()

tmp4 <- get_corrections(df_doubles, dftest, 1, 500, 8)
tmp4[duplicated(tmp4$bad_id),]

foo <- df[df$siteaddid %in% tmp4$bad_id,] %>% 
  mutate(change=paste("Corrected STREET_NAME to",
                      tmp4[siteaddid %in% tmp4$bad_id,]$STREET_NAME))
#   Add corrections for singles
corrections <- bind_rows(corrections, foo)
    

#------ Now update the original by adding a new field

dfnew <- dfnew %>% mutate(New_street=roadname)

dfnew[dfnew$siteaddid %in% tmp$bad_id,]$New_street <- tmp$STREET_NAME 
dfnew[dfnew$siteaddid %in% tmp2$bad_id,]$New_street <- tmp2$STREET_NAME 
dfnew[dfnew$siteaddid %in% tmp3$bad_id,]$New_street <- tmp3$STREET_NAME 
dfnew[dfnew$siteaddid %in% tmp4$bad_id,]$New_street <- tmp4$STREET_NAME 

##########    Let's look at ST vs. SAINT

eraseme <- 
df %>% as_tibble() %>%
  filter(str_detect(roadname, "^SAINT |^ST ")) %>%
  mutate(pre=str_extract(roadname, "^SAINT |^ST ")) %>% 
  mutate(post=str_remove(roadname, "^SAINT |^ST ")) %>% 
  group_by(post, pre, zipcode) %>% 
    summarize(n=n())

#   We will declare that all saint streets will begin with the word spelled out.

##########    Now fix a few specific problems that appear

#   Farm Road will be FM

Old_street <- c("H M C", "^F M ", "^F.M. ",
                "^TRAMMEL-" ,"^AVE " ,"^H R$" ,"^T K C$", " SQ$",
                "^ST ")
New_street <- c("H MARK CROSSWELL JR", "FM ", "FM ", 
                "TRAMMEL ", "AVENUE ", "H AND R", "TKC", " SQUARE",
                "SAINT ")

dfnew[dfnew$New_street=="H M C",]$New_street <- "H MARK CROSSWELL JR"
dfnew[dfnew$New_street=="2136TH F M 2920",]$New_street <- "FM 2920"
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^F M ", "FM "))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^F.M. ", "FM "))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^TRAMMEL-$", "TRAMMEL "))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^AVE ", "AVENUE "))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^H R$", "H AND R"))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^T K C$", "TKC"))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street," SQ$", " SQUARE"))
dfnew <- dfnew %>% mutate(New_street=str_replace(New_street,"^ST ", "SAINT "))

dfnew <- dfnew %>% 
  filter(!roadname=="") #   Some roads have no names. Centerpoint poles?

ObjID <- df %>% filter(str_detect(roadname, 
                                  paste(Old_street, collapse="|")))  

foo <-  df[df$siteaddid %in% ObjID$siteaddid,] %>% 
  mutate(change=paste("Corrected street name to", 
                       dfnew[dfnew$siteaddid %in% ObjID$siteaddid,]$New_street)) 
corrections <- bind_rows(corrections, foo)

#   Save all our hard work to an intermediate file, just in case.

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_1"))
saveRDS(corrections, paste0(path, "corrections.Rds"))

```

```{r reload}

dfnew <- readRDS(paste0(path, "temporary_intermediate_file_1"))

```

```{r suffixes}
dfnew <- dfnew %>% 
  mutate(suffix=na_if(suffix, "")) # turn empty into NA for consistency

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  filter(suffix!="") %>% 
  select(suffix) %>% 
  unique

#   No apparent issues!

# foo <- df[!is.na(df$SUFFIX) & str_detect(df$SUFFIX, "[^NSEW]"),] %>% 
#   mutate(change="Corrected SUFFIX by deleting bogus value")
# 
# corrections <- bind_rows(corrections, foo)
# 
# dfnew$SUFFIX[!is.na(dfnew$SUFFIX) & str_detect(dfnew$SUFFIX, "[^NSEW]")] <- NA

```

```{r street type}

#   First get rid of blanks since they just gum up the works

dfnew <- dfnew %>% 
  mutate(roadtype=stringr::str_trim(roadtype)) %>% 
  mutate(roadtype=na_if(roadtype, "")) # turn empty into NA for consistency
  

dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  filter(roadtype!="") %>% 
  select(roadtype) %>% 
  unique

Street_types <- 
dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
      group_by(roadtype) %>%
         summarize(n=n(),
                   roadtype=last(roadtype)) %>% 
  arrange(-n)

#   a lot of mess here. Will use a similar strategy as used for street names.
#   look at nearby friends and try to grab the type from them. Use total dataset
#   n value to establish the "standard".
#   Two tasks - look for missing types by checking nearby friends
#   And check the existing type against the standards

Allowed_types <- c(
  "ALY", "ANX", "ARC", "AVE", "BYU", "BCH", "BND", "BLF", "BLFS", "BTM", "BLVD",
  "BR", "BRG", "BRK", "BRKS", "BG", "BGS", "BYP", "CP", "CYN", "CPE", "CSWY", "CTR",
  "CTRS", "CIR", "CIRS", "CLF", "CLFS", "CLB", "CMN", "CMNS", "COR", "CORS",
  "CRSE", "CT", "CTS", "CV", "CVS", "CRK", "CRES", "CRST", "XING", "XRD", "XRDS",
  "CURV", "DL", "DM", "DV", "DR", "DRS", "EST", "ESTS", "EXPY", "EXT", "EXTS", "FALL",
  "FLS", "FRY", "FLD", "FLDS", "FLT", "FLTS", "FRD", "FRDS", "FRST", "FRG", "FRGS",
  "FRK", "FRKS", "FT", "FWY", "GDN", "GDNS", "GTWY", "GLN", "GLNS", "GRN", "GRNS",
  "GRV", "GRVS", "HBR", "HBRS", "HVN", "HTS", "HWY", "HL", "HLS", "HOLW", "INLT",
  "IS", "ISS", "ISLE", "JCT", "JCTS", "KY", "KYS", "KNL", "KNLS", "LK", "LKS",
  "LAND", "LNDG", "LN", "LGT", "LGTS", "LF", "LCK", "LCKS", "LDG", "LOOP", "MALL",
  "MNR", "MNRS", "MDW", "MDWS", "MEWS", "ML", "MLS", "MSN", "MTWY", "MT", "MTN",
  "MTNS", "NCK", "ORCH", "OVAL", "OPAS", "PARK", "PKWY", "PASS", "PSGE", "PATH",
  "PIKE", "PNE", "PNES", "PL", "PLN", "PLNS", "PLZ", "PT", "PTS", "PRT", "PRTS",
  "PR", "RADL", "RAMP", "RNCH", "RPD", "RPDS", "RST", "RDG", "RDGS", "RIV", "RD",
  "RDS", "RTE", "ROW", "RUE", "RUN", "SHL", "SHLS", "SHR", "SHRS", "SKWY", "SPG",
  "SPGS", "SPUR", "SQ", "SQS", "STA", "STRA", "STRM", "ST", "STS", "SMT", "TER",
  "TRWY", "TRCE", "TRAK", "TRFY", "TRL", "TRLR", "TUNL", "TPKE", "UPAS", "UN",
  "UNS", "VLY", "VLYS", "VIA", "VW", "VWS", "VLG", "VLGS", "VL", "VIS", "WALK",
  "WALL", "WAY", "WAYS", "WL", "WLS")


#   Let's look for erroneous types

Street_types %>% filter(!is.na(roadtype)) %>% 
  filter(!roadtype %in% Allowed_types)

#   Well that is simple enough. In Houston, apparently, "Speedway" is a street
#   type, so I'll leave that one alone. The rest I can update to the correct
#   value.

foo <- df %>% 
  filter(str_detect(roadtype, "^PKY$|^ROAD$|^LANE$|^TWY$")) %>% 
  mutate(change="Corrected roadtype to standard USPS values (except SPWY)")

corrections <- bind_rows(corrections, foo)

dfnew <- dfnew %>% 
  mutate(roadtype=str_replace(roadtype, "^PKY$", "PKWY")) %>% 
  mutate(roadtype=str_replace(roadtype, "^ROAD$", "RD")) %>% 
  mutate(roadtype=str_replace(roadtype, "^LANE$", "LN")) %>% 
  mutate(roadtype=str_replace(roadtype, "^TWY$", "CT"))  # not a tollway
  
#   Look at the NA's to see if they make sense

#   For each street/zip pair sum up # types

tmp <- 
dfnew %>% sf::st_drop_geometry() %>% # This will speed things considerably
  group_by(zipcode, roadname) %>% 
      summarise(n=n(),
                sumNA = sum(is.na(roadtype)),
                NotNAs=list(unique(roadtype[!is.na(roadtype)]))) %>% 
  filter(!sumNA==n) %>% 
  mutate(NotNA=n-sumNA) %>% 
  mutate(Flag=lengths(NotNAs)) %>% 
  filter(sumNA>0) %>% 
  mutate(NotNAs=paste(NotNAs)) %>% # convert list to string 
  filter(Flag==1)  # remove ambiguous cases where there are multiple types

#   Let's first fix things that are not NA that should be
#   

#   Add new field so we retain original values

dfnew <- dfnew %>% mutate(old_roadtype = roadtype) 

#   These will be the records we set the roadtype field to NA for
tmp_rm_not_na <- tmp %>% 
  filter(n>9) %>% # need at least 10 to believe statistics
  mutate(foo=(NotNA/n)) %>% 
  filter((NotNA/n)<0.151) %>%  # arbitrary cutoff from eyeballing data
  filter(!NotNAs=="CT") # don't trust 'court', leave alone

#   Apply what is left to NA that field

for (i in 1:nrow(tmp_rm_not_na)){
  dfnew$roadtype[(dfnew$zipcode==tmp_rm_not_na[[i,1]]) & 
          (dfnew$roadname==tmp_rm_not_na[[i,2]])] <- NA
}

ObjID <- dfnew %>% 
  filter(is.na(roadtype)) %>% 
  filter(!is.na(old_roadtype))

foo <-  df[df$siteaddid %in% ObjID$siteaddid,] %>% 
  mutate(change="Removed roadtype") 
corrections <- bind_rows(corrections, foo)
  
# Now let's fix things that are NA but shouldn't be

tmp_rm_na <- tmp %>% 
  filter(n>9) %>% # need at least 10 to believe statistics
  mutate(foo=(NotNA/n)) %>% 
  filter((NotNA/n)>0.730) %>% 
  filter(!NotNAs=="CT") %>% # don't trust 'court', leave alone
  filter(!NotNAs=="CIR") %>% # don't trust 'circle', leave alone
  filter(!NotNAs=="PL") # don't trust 'place', leave alone

#   Apply what is left to fill that field

for (i in 1:nrow(tmp_rm_na)){
  dfnew$roadtype[(dfnew$zipcode==tmp_rm_na[[i,1]]) & 
          (dfnew$roadname==tmp_rm_na[[i,2]])] <- tmp_rm_na[[i,5]]
}
  
ObjID <- dfnew %>% 
  filter(!is.na(roadtype)) %>% 
  filter(is.na(old_roadtype))

foo <-  df[df$siteaddid %in% ObjID$siteaddid,] %>% 
  mutate(change=paste("Added roadtype of", 
                       dfnew[dfnew$siteaddid %in% ObjID$siteaddid,]$roadtype)) 
corrections <- bind_rows(corrections, foo)

#   Save all our hard work to an intermediate file, just in case.

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_2"))
saveRDS(corrections, paste0(path, "corrections_2.Rds"))

```

##  Prefix redux

Many prefixes are missing. We will try to use nearby address points to fill in
the Prefix where it seems safe to do so.



```{r revisit prefix}


#####################   stopped here #######################

#   Can we grab nearby prefixes to repair missing ones?

#   Let's shrink dataset size
dftest <- 
dfnew %>% 
  select(OBJECTID, Street_num, Street_alpha, PREFIX, New_street, 
         Street_type, ZIPCODE, X_COORD, Y_COORD) 

#   Now start analysis by street name, type, and zipcode

tmp <- 
dftest %>% group_by(ZIPCODE, New_street, Street_type) %>% 
      summarise(n=n(),
                sumNA = sum(is.na(PREFIX)),
                sumE = sum(PREFIX=="E", na.rm=TRUE),
                sumW = sum(PREFIX=="W", na.rm=TRUE),
                sumN = sum(PREFIX=="N", na.rm=TRUE),
                sumS = sum(PREFIX=="S", na.rm=TRUE)) %>%
  mutate(singlet = n-sumNA) %>% 
  filter(!sumNA==n) %>% 
  filter(sumNA>0) %>% 
  mutate(percentNA = sumNA/n) 

#   Take singlet prefixes with total n>10 and find minimum distance to
#   a cousin

#   data frame of single occurrence PREFIXes
df_singles <- tmp %>% 
  filter(n>10) %>% 
  filter(singlet==1) %>% 
  filter(!Street_type == "CIR") %>% # eliminate circles
  filter(!Street_type == "CT") %>%   # eliminate courts
  mutate(nearest=NA)

for (i in 1:nrow(df_singles)) {
  tmp_sing <- dftest %>% filter((ZIPCODE==df_singles[i,]$ZIPCODE) &
                                (New_street==df_singles[i,]$New_street) & 
                                (Street_type==df_singles[i,]$Street_type))
  
  bad_X <- tmp_sing[!is.na(tmp_sing$PREFIX),]$X_COORD
  bad_Y <- tmp_sing[!is.na(tmp_sing$PREFIX),]$Y_COORD
  
  tmp_sing <- tmp_sing %>% 
    mutate(dist=sqrt((X_COORD - bad_X)^2 +
                     (Y_COORD-bad_Y)^2)) %>% 
    arrange(dist)
  
  df_singles[i,]$nearest <- tmp_sing[2,]$dist
}

#   Let's use a 150 foot cutoff. Further away than that and I'll ignore it.

df_singles <- df_singles %>% filter(nearest<150)

dfnew <- dfnew %>% mutate(Prefix=PREFIX)
  
for (i in 1:nrow(df_singles)){
  dfnew[(dfnew$New_street %in% df_singles[i,]$New_street) &
        (!is.na(dfnew$PREFIX)) &
        (dfnew$ZIPCODE %in% df_singles[i,]$ZIPCODE) &
        (dfnew$Street_type %in% df_singles[i,]$Street_type),]$Prefix <- NA
}

ObjID <- dfnew %>% filter(!is.na(PREFIX), # who did we change?
                 is.na(Prefix))   

foo <-  df[df$OBJECTID %in% ObjID$OBJECTID,] %>% 
  mutate(change="Removed PREFIX") 
corrections <- bind_rows(corrections, foo)

#######   now let's look at NA's that shouldn't be.
#   This is more complicated since the needed prefix could be E or W say.
#   About 0.1 for the percent cutoff seems reasonable

#   data frame of candidates
df_cands <- tmp %>% 
  filter(n>10) %>% 
  filter(percentNA<0.1) %>% 
  filter(!Street_type == "CIR") %>% # eliminate circles
  filter(!Street_type == "CT")  # eliminate courts

#   Expand this out to all the individual points in prep for distance calc
#   Let's shrink dataset size
dftest <- 
dfnew %>% 
  select(OBJECTID, Street_num, Street_alpha, Prefix, New_street, 
         Street_type, ZIPCODE, X_COORD, Y_COORD) 
dftest$Nearest <- 0
dftest$Minimus <- 0
dftest$Neighbor_object <- NA

for (i in 1:nrow(df_cands)){
#for (i in 1:3){
  print(paste(i, "---->", df_cands[i,]$New_street))
  eraseme <- dftest %>% 
    filter((ZIPCODE %in% df_cands[i,]$ZIPCODE) &
           (New_street %in% df_cands[i,]$New_street) &
           (Street_type %in% df_cands[i,]$Street_type))  
  
  targets <- eraseme %>% filter(is.na(Prefix)) %>% 
    mutate(Prefix=NA,
           dist=0)
  
  for (j in 1:nrow(targets)){
    foo <-   eraseme %>% 
    summarize(nearest=sqrt((X_COORD -targets[j,]$X_COORD)^2 +
                        (Y_COORD-targets[j,]$Y_COORD)^2),
              prefix=Prefix,
              object=OBJECTID) %>% 
      arrange(nearest) %>% 
      drop_na()
    
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Prefix <- foo[1,]$prefix
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Nearest <- foo[1,]$nearest
    dftest[targets[j,]$OBJECTID == dftest$OBJECTID,]$Neighbor_object <- foo[1,]$object
    #print("--b--")
  }
}

dftest %>% filter(Nearest>0,
                 Nearest<300) %>% 
  ggplot() +
  geom_histogram(aes(x=Nearest))

#   150 ft feels like the furthest I can do with confidence.

foo <- dftest %>% filter(Nearest<150,
                         Nearest>0)
                   
dfnew[dfnew$OBJECTID %in% foo$OBJECTID,]$Prefix <- foo$Prefix

ObjID <- dfnew %>% filter(is.na(PREFIX), # who did we change?
                          !is.na(Prefix))   

foo <-  df[df$OBJECTID %in% ObjID$OBJECTID,] %>% 
  mutate(change="Added PREFIX based on neighbors") 
corrections <- bind_rows(corrections, foo)

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_3"))
saveRDS(corrections, paste0(path, "corrections_3.Rds"))
```


```{r City}

dfnew$CITY %>% unique() %>% sort()


```

City names look okay.

```{r zips}

dfnew$ZIPCODE %>% unique() %>% sort()

```

Zipcodes look okay.

```{r sources}

dfnew$SOURCES %>% unique() %>% sort()


```

A little ambiguity between "COH" and "City of Houston", also CP and Centerpoint
Energy, Center Point, and CE. And some other miscellaneous cleanup.
```{r last edit date}

dfnew %>% mutate(date=lubridate::ymd_hms(last_edited_date)) %>% 
  ggplot() +
  geom_histogram(aes(x=date))

dfnew$Street_num <- as.character(dfnew$Street_num)
dfnew <- dfnew %>% replace_na(list(Prefix="", Street_type=""))
dfnew <- dfnew %>% rename(Street_name=New_street)
dfnew <- dfnew %>% mutate(CITY=str_to_upper(CITY))

```

No last edited dates before 2015.

```{r}
#   State Plane, Texas South Central Zone 5401, FIPS 4204, NAD83, EPSG 2278
#   There are Y values missing...
dftest <- 
dfnew %>% 
  #head(10) %>% 
  select(OBJECTID, STREET_NAME, Street_num, ZIPCODE, X, Y, X_COORD, Y_COORD) %>% 
  #select(OBJECTID, STREET_NAME, Street_num, ZIPCODE, X_COORD, Y_COORD)# %>% 
  sf::st_as_sf(., coords=c("X", "Y"))

sf::st_crs(dftest) <- googlecrs

foo <- sf::st_transform(dftest, crs="EPSG:2278")

#foo <- 
foo %>% 
  mutate(xy=sf::st_coordinates(foo))

foo2 <- sf::st_coordinates(foo)
foo$X <- foo2[,1]
foo$Y <- foo2[,2]

foo %>% mutate(Dx=X_COORD-X,
               Dy=Y_COORD-Y) %>% 
  filter(Dy>-5) %>% 
  filter(Dy<0) %>% 
  ggplot(aes(x=Dy)) +
  geom_histogram()

foo %>% mutate(Dx=X_COORD-X,
               Dy=Y_COORD-Y) %>% 
  filter(Dx>0) %>% 
  filter(Dx<5) %>% 
  ggplot(aes(x=Dx)) +
  geom_histogram()

#     Looks good enough. Let's recover the data with bogus Y coordinates.

ObjID <- dfnew %>% 
  filter(Y_COORD<13000000) # pull out records with bad Y coordinate

foo[foo$OBJECTID %in% ObjID$OBJECTID,]

dfnew[dfnew$OBJECTID %in% ObjID$OBJECTID,]$Y_COORD <- 
  unname(foo[foo$OBJECTID %in% ObjID$OBJECTID,]$Y)

foo <-  df[df$OBJECTID %in% ObjID$OBJECTID,] %>% 
  mutate(change="Repaired Y_COORD by reprojecting") 
corrections <- bind_rows(corrections, foo)

#   Safety save

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_4"))
saveRDS(corrections, paste0(path, "corrections_4.Rds"))

```

##        More zipcode stuff. Some are just plain wrong

Gather data that should be the same address and calculate the max diagonal
distance for each set of "duplicates" (we ignore apartment/building numbers,
so expect duplicates)

For distances that seem unreasonable, intersect the lat/long with the zipcode
polygons and look for bad ones.

If I can, correct the zipcode. If I can't, flag the point as hopeless. 

```{r more zip}
df_nodups <- 
dfnew %>% #head(20000) %>% 
  group_by(ZIPCODE, Street_name, Street_num, Street_type, Prefix) %>% 
  summarise(n=n(),
            Street_num=last(Street_num),
            Prefix=last(Prefix),
            Street_name=last(Street_name),
            Street_type=last(Street_type),
            ZIPCODE=tail(names(sort(table(ZIPCODE))),1),
            Lat=mean(Y),
            Lon=mean(X),
            Xmax=(max(X_COORD)-min(X_COORD)),
            Ymax=(max(Y_COORD)-min(Y_COORD)),
            Diagonal=sqrt(Xmax**2 + Ymax**2),
            Xstd=sd(X_COORD),
            Ystd=sd(Y_COORD),
            X=mean(X_COORD),
            Y=mean(Y_COORD)
            ) %>% 
  select(Street_num, Prefix, Street_name, Street_type, ZIPCODE,
         n, Diagonal, Xmax, Ymax, Xstd, Ystd, X, Y, Lat, Lon) %>% 
  filter(n>1)

#   Now let's intersect the df_nodups file, where the diagonal distance >500 ft
#   with the zipcode polygons. I don't know which polygon file CoH uses, so I
#   will use a recent one (2019) from the state.

#   All texas zips

temp <- readRDS(file = "/home/ajackson/Dropbox/Rprojects/Curated_Data_Files/Zipcodes/COH_Zip_Polys.rds")

temp <- temp %>% rename(Zip=ZIP_CODE) %>% 
  mutate(Zip=as.character(Zip))

# Prep coordinate file

#   First pull out appropriate records from df_nodups

tmp <- df_nodups %>% filter(Diagonal>500) %>% # let's start with the worst ones
  select(Street_num, Prefix, Street_name, Street_type, ZIPCODE, Diagonal, n) %>% 
  inner_join(dfnew,by=c("Street_num", "Prefix", "Street_name", 
                        "Street_type", "ZIPCODE"))

googlecrs <- 4326
dat <- data.frame(Longitude=tmp$X, Latitude=tmp$Y, stringsAsFactors = FALSE)

dat <- sf::st_as_sf(dat, coords=c("Longitude", "Latitude"), crs=googlecrs, agr = "identity")

sf::sf_use_s2(FALSE)

#   find points in polygons
#   since zipcodes don't overlap, let's just grab the array index
#   instead of creating a huge matrix
a <- sf::st_intersects(dat, temp, sparse = TRUE)

#   Append the zipcode field to the data frame
tmp$Zip <- temp$Zip[unlist(a)]

#   Look for Zip != Zipcode

#   This represents all duplicate addresses that contain 2 or more zipcodes
eraseme <- 
tmp %>% mutate(Diagonal=as.character(Diagonal)) %>% 
  group_by(Diagonal) %>% 
    mutate(numzips=length(unique(Zip))) %>% 
  ungroup() %>% 
  filter(numzips>1) %>% # This gets rid of apartments and the like
  mutate(Diagonal=as.numeric(Diagonal))

#   Check bad zipcodes. Either they make no sense, or they might. Test by looking
#   nearby for the street name

#   More apartment insurance
Bad_zips <- eraseme %>% filter(ZIPCODE!=Zip, n<10)

#   Make an sf object
Baddat <- sf::st_as_sf(Bad_zips, coords=c("X", "Y"), crs=googlecrs, agr = "identity")
Bad_zips$hits <- 0 # How many addresses with same street name live in new zip?
Bad_zips$closeness <- 0  # how close am I to the right zipcode?
for (i in 1:nrow(Bad_zips)){
  print(paste("---", i, "---", Bad_zips[i,]$Street_name))
  Bad_zips[i,]$hits <- 
    sum(str_detect(dfnew$ZIPCODE,Bad_zips[i,]$Zip) & 
        str_detect(dfnew$Street_name,Bad_zips[i,]$Street_name)
  )
#   How close to bad zipcode is the point? Boundary of zip may have moved.
  Bad_zips[i,]$closeness <- as.numeric(sf::st_distance(Baddat[i,], 
                            temp[str_detect(temp$Zip,Bad_zips[i,]$ZIPCODE),]))
}

#   For closeness of < 250 feet, leave the record alone.

Bad_zips <- Bad_zips %>% 
  filter(closeness>250)

#   for hits < 3, delete the record

ObjID <- Bad_zips[Bad_zips$hits<3,]$OBJECTID

foo <-  df[df$OBJECTID %in% ObjID,] %>% 
  mutate(change="Location mismatches Zipcode so badly point discarded") 
corrections <- bind_rows(corrections, foo)

#   Delete record
for (i in 1:nrow(Bad_zips)){
  if (Bad_zips[i,]$hits<3) {
    dfnew <- dfnew[dfnew$OBJECTID!=Bad_zips[i,]$OBJECTID,]
  }
}
  
#   for hits >=2, correct the zipcode
ObjID <- Bad_zips[Bad_zips$hits>=3,]$OBJECTID

foo <-  df[df$OBJECTID %in% ObjID,] %>% 
  mutate(change=paste("Location mismatches Zipcode, correct it to", 
                       Bad_zips[Bad_zips$OBJECTID %in% ObjID,]$Zip)) 

foo2 <- Bad_zips[Bad_zips$OBJECTID %in% ObjID,] %>% 
  select(OBJECTID, Zip)
foo <- left_join(df[df$OBJECTID %in% ObjID,],
                 foo2,
                 by="OBJECTID") %>% 
  mutate(change=paste("Location mismatches Zipcode, correct it to", Zip )) %>% 
  select(-Zip)

corrections <- bind_rows(corrections, foo)

Bad_zips <- Bad_zips %>% filter(hits>=3)
for (i in 1:nrow(Bad_zips)){
    dfnew[dfnew$OBJECTID==Bad_zips[i,]$OBJECTID,]$ZIPCODE <- Bad_zips[i,]$Zip 
}

#     Let's look at dups again

df_nodups <- 
dfnew %>% #head(20000) %>% 
  group_by(ZIPCODE, Street_name, Street_num, Street_type, Prefix) %>% 
  summarise(n=n(),
            Street_num=last(Street_num),
            Prefix=last(Prefix),
            Street_name=last(Street_name),
            Street_type=last(Street_type),
            ZIPCODE=tail(names(sort(table(ZIPCODE))),1),
            Lat=mean(Y),
            Lon=mean(X),
            Xmax=(max(X_COORD)-min(X_COORD)),
            Ymax=(max(Y_COORD)-min(Y_COORD)),
            Diagonal=sqrt(Xmax**2 + Ymax**2),
            Xstd=sd(X_COORD),
            Ystd=sd(Y_COORD),
            X=mean(X_COORD),
            Y=mean(Y_COORD)
            ) %>% 
  select(Street_num, Prefix, Street_name, Street_type, ZIPCODE,
         n, Diagonal, Xmax, Ymax, Xstd, Ystd, X, Y, Lat, Lon) %>% 
  filter(n>1)

#   Still needs work, but fixing more prefixes should help.
#   Safety save

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_5"))
saveRDS(corrections, paste0(path, "corrections_5.Rds"))

dfnew <- readRDS(paste0(path, "temporary_intermediate_file_5"))
corrections <- readRDS(paste0(path, "corrections_5.Rds"))

```

```{r more prefixes}

dfnew <- dfnew %>% 
  mutate(Zipcode=ZIPCODE)  
  
df_prefix <- dfnew %>% 
  group_by(Zipcode, Street_name, Street_type) %>% 
    summarize(n=n(),
              sumNA = sum(Prefix==""),
              sumE = sum(Prefix=="E", na.rm=TRUE),
              sumW = sum(Prefix=="W", na.rm=TRUE),
              sumN = sum(Prefix=="N", na.rm=TRUE),
              sumS = sum(Prefix=="S", na.rm=TRUE)) %>%
    filter(sumNA<n) %>% # These are okay so drop them 
    filter(n>5) %>% # Too little information
    filter(n>5*sumNA) # Need plenty of good data

#   Do some exploration

bad_pts <- 
df_prefix %>% filter(sumNA>0) #%>%  arrange(-n) %>%  head(10)
#   Subset dfnew based on these guys
df_bad <- inner_join(dfnew, bad_pts, 
                     by=c("Zipcode", "Street_name", "Street_type"))

Save_stats <- NULL
df_worse <- df_bad[df_bad$Prefix=="",]
####df_worse <- df_worse %>% filter(Street_name=="PEACEFUL CANYON")
for (i in 1:nrow(df_worse)){
  pt <- df_worse[i,]
  print(paste("----", pt$Street_num, pt$Street_name, pt$Street_type))
  eraseme <-  df_bad %>% 
    filter(Street_name==pt$Street_name,
           Street_type==pt$Street_type) %>% 
    mutate(eucdist=sqrt((X_COORD - pt$X_COORD)^2 +
                          (Y_COORD - pt$Y_COORD)^2)) %>% 
    filter(eucdist <300,
           !Prefix=="") %>% 
  #   How many candidates do I get within 300 feet?
    group_by(Prefix) %>%
      summarize(num_pts=n(),
                n=last(n),
              sumE = sum(Prefix=="E"),
              sumW = sum(Prefix=="W"),
              sumN = sum(Prefix=="N"),
              sumS = sum(Prefix=="S"),
              Street_name=last(Street_name)
                ) %>%
    mutate(OBJECTID=pt$OBJECTID, 
           Street_num=pt$Street_num)
 #  If there is more than one prefix in the vicinity, don't do anything
  if (nrow(eraseme)==1){
    Save_stats <- rbind(Save_stats, eraseme)
  }
}

#     Correct spots that have 5 or more neighbors within 300 feet that all have
#     the same Prefix

Save_stats <- Save_stats %>% 
  filter(num_pts >= 5) %>% 
  select(OBJECTID, Prefix_new=Prefix)

#   Update corrections and dfnew

ObjID <- Save_stats$OBJECTID
foo <- left_join(df[df$OBJECTID %in% ObjID,],
                 Save_stats,
                 by="OBJECTID") %>% 
  mutate(change=paste("Missing Prefix updated based on nearby points to",
                      Prefix_new )) %>% 
  select(-Prefix_new)

corrections <- bind_rows(corrections, foo)

dfnew[dfnew$OBJECTID %in% ObjID,]$Prefix <- Save_stats$Prefix_new

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_6"))
saveRDS(corrections, paste0(path, "corrections_6.Rds"))
##-----------------------------   stop here

#   Let's take another swipe at it

df_prefix <- dfnew %>% 
  group_by(Zipcode, Street_name, Street_type) %>% 
    summarize(n=n(),
              sumNA = sum(Prefix==""),
              sumE = sum(Prefix=="E", na.rm=TRUE),
              sumW = sum(Prefix=="W", na.rm=TRUE),
              sumN = sum(Prefix=="N", na.rm=TRUE),
              sumS = sum(Prefix=="S", na.rm=TRUE)) %>%
    filter(sumNA<n) %>% # These are okay so drop them 
    filter(n>5) %>% # Too little information
    filter(n>5*sumNA) # Need plenty of good data

#   Do some exploration

bad_pts <- 
df_prefix %>% filter(sumNA>0) #%>%  arrange(-n) %>%  head(10)
#   Subset dfnew based on these guys
df_bad <- inner_join(dfnew, bad_pts, 
                     by=c("Zipcode", "Street_name", "Street_type"))

Save_stats <- NULL
df_worse <- df_bad[df_bad$Prefix=="",]
####df_worse <- df_worse %>% filter(Street_name=="PEACEFUL CANYON")
for (i in 1:nrow(df_worse)){
  pt <- df_worse[i,]
  print(paste("----", pt$Street_num, pt$Street_name, pt$Street_type))
  eraseme <-  df_bad %>% 
    filter(Street_name==pt$Street_name,
           Street_type==pt$Street_type) %>% 
    mutate(eucdist=sqrt((X_COORD - pt$X_COORD)^2 +
                          (Y_COORD - pt$Y_COORD)^2)) %>% 
    filter(eucdist <500,
           !Prefix=="") %>% 
  #   How many candidates do I get within 500 feet?
    group_by(Prefix) %>%
      summarize(num_pts=n(),
                n=last(n),
              sumE = sum(Prefix=="E"),
              sumW = sum(Prefix=="W"),
              sumN = sum(Prefix=="N"),
              sumS = sum(Prefix=="S"),
              Street_name=last(Street_name)
                ) %>%
    mutate(OBJECTID=pt$OBJECTID, 
           Street_num=pt$Street_num)
 #  If there is more than one prefix in the vicinity, don't do anything
  if (nrow(eraseme)==1){
    Save_stats <- rbind(Save_stats, eraseme)
  }
}

#     Correct spots that have 10 or more neighbors within 500 feet that all have
#     the same Prefix

Save_stats <- Save_stats %>% 
  filter(num_pts >= 10) %>% 
  select(OBJECTID, Prefix_new=Prefix)
  
#   Update corrections and dfnew

ObjID <- Save_stats$OBJECTID
foo <- left_join(df[df$OBJECTID %in% ObjID,],
                 Save_stats,
                 by="OBJECTID") %>% 
  mutate(change=paste("Missing Prefix updated based on 10/500 nearby points to",
                      Prefix_new )) %>% 
  select(-Prefix_new)

corrections <- bind_rows(corrections, foo)

dfnew[dfnew$OBJECTID %in% ObjID,]$Prefix <- Save_stats$Prefix_new

saveRDS(dfnew, paste0(path, "temporary_intermediate_file_7"))
saveRDS(corrections, paste0(path, "corrections_7.Rds"))

#####Save_stats[duplicated(Save_stats$OBJECTID),]

```

##   Fix some specific errors manually

```{r manual}

dfnew <- readRDS(paste0(path, "temporary_intermediate_file_7"))
corrections <- readRDS(paste0(path, "corrections_7.Rds"))

eraseme <- dfnew %>% 
  mutate(pseudotype=str_extract(Street_name, "\\w+$")) %>% 
  filter(pseudotype==Street_type) %>% 
  group_by(Street_name) %>% 
    summarize(Street_type=last(Street_type),
              n=n())
#   Martin Luther King consistently missing Jr at end
#   Baron Brook Dr Dr not sensible, etc

Old_street <- c("MARTIN LUTHER KING$", "BARON BROOK DR", "BROKEN BACK DR STE",
                "COLBURN DR N", "SAGEBROOK DR", "LINBROOK DRIVE", 
                "HAYBROOK DRIVE", "SANOUR DR", "FAIRWAY DR STE",
                "SHERWOOD RIDGE DRIVE", "BRITTMOORE PARK DRIV", 
                "CANDLELIGHT PLACE DR", "STEINHAGEN RD", "BLACK FALCON RD",
                "NICHOLSON ST", "SINGLE OAK ST", "PALSTON BEND LN",
                "SIENNA COVE LN", "LA PORTE FWY", "COMETS RUN",
                "OAK RIDGE DRIVE", "BEMBRIDGE DRIVE", "PAXTON DRIVR",
                "GILBERT SCOTT DR", "GREENS LANDING DR", 
                "HAVEN CREEK DR/BACKS", "MAGIC FALLS DRIVE",
                "BLUE BONNET DRIVE", "FALLBROOKD DR", "CYPRESS VALLEY DR",
                "CLIFF PARK DRIVE", "WHITE OAK RIDGE DR", "IMPERIAL CREEK DRIVE",
                "LAZY CREEK DR", "KILBURN RD", "FRY RD", "MITCHELL RD",
                "BARKALOO RD STE", "SHERIDAN RD", "ROBERT RD ROBERT",
                "MALCOLMSON RD", "PIN OAK RD", "MONDAY HARGR 0VE RD",
                "SMITHFIELD CROSSING LN", "DA VINA LN", "SCROGGINS LN",
                "1510TH BEVIS ST", "PECAN ST STE", "LEADER ST", "SAGECIRCLE ST 1",
                "CENTRAL AVE", "FRANK SCOTT BLVD", "ALCEA CT", "CLIFF MARSHAL ST",
                "COUNTRY VILLAGE BLVD", "CREEK CIR", "FRANK SCOTT BLVD",
                "HOBBY AIRPORT LOOP", "MOSSY OAK DR", "MOUNTAIN MAPLE CT",
                "OLD MAIN LOOP", "PETINA CYPRESS CT")

New_street <- c("MARTIN LUTHER KING JR", "BARON BROOK", "BROKEN BACK",
                "COLBURN", "SAGEBROOK", "LINBROOK",
                "HAYBROOK", "SANOUR", "FAIRWAY",
                "SHERWOOD RIDGE", "BRITTMOORE PARK",
                "CANDLELIGHT PLACE", "STEINHAGEN", "BLACK FALCON",
                "NICHOLSON", "SINGLE OAK", "PALSTON BEND",
                "SIENNA COVE", "LA PORTE", "COMETS",
                "OAK RIDGE", "BEMBRIDGE", "PAXTON",
                "GILBERT SCOTT", "GREENS LANDING",
                "HAVEN CREEK", "MAGIC FALLS",
                "BLUE BONNET", "FALLBROOK", "CYPRESS VALLEY",
                "CLIFF PARK", "WHITE OAK RIDGE", "IMPERIAL CREEK",
                "LAZY CREEK", "KILBURN", "FRY", "MITCHELL",
                "BARKALOO", "SHERIDAN", "ROBERT",
                "MALCOLMSON", "PIN OAK", "MONDAY HARGROVE", 
                "SMITHFIELD CROSSING", "DA VINA", "SCROGGINS",
                "BEVIS", "PECAN", "LEADER", "SAGECIRCLE",
                "CENTRAL", "FRANK SCOTT", "ALCEA", "CLIFF MARSHAL",
                "COUNTRY VILLAGE", "CREEK", "FRANK SCOTT",
                "HOBBY AIRPORT", "MOSSY OAK", "MOUNTAIN MAPLE",
                "OLD MAIN", "PETINA CYPRESS")

#  First repair type where needed

dfnew[str_detect(dfnew$Street_name, "FAIRWAY DR STE"),]$Street_type <- "DR"
dfnew[str_detect(dfnew$Street_name, "PAXTON DRIVR"),]$Street_type <- "DR"
dfnew[str_detect(dfnew$Street_name, "HAVEN CREEK DR/BACKS"),]$Street_type <- "DR"
dfnew[str_detect(dfnew$Street_name, "BLACK FALCON RD"),]$Street_type <- "RD"
dfnew[str_detect(dfnew$Street_name, "LA PORTE FWY"),]$Street_type <- "FWY"
dfnew[str_detect(dfnew$Street_name, "FRANK SCOTT BLVD"),]$Street_type <- "BLVD"

#   Now repair names

for (i in 1:length(Old_street)) {
  dfnew[str_detect(dfnew$Street_name,Old_street[i]),]$Street_name <- New_street[i]
}

ObjID <- dfnew %>% filter(str_detect(STREET_NAME, 
                                  paste(Old_street, collapse="|")))  

foobar <-  df[df$OBJECTID %in% ObjID$OBJECTID,] %>% 
  mutate(change=paste("Corrected street name to", 
                       dfnew[dfnew$OBJECTID %in% ObjID$OBJECTID,]$Street_name)) 
corrections <- bind_rows(corrections, foobar)
saveRDS(corrections, paste0(path, "Final_corrections.Rds"))

```


```{r averaging}
#   Complete file after cleanup
saveRDS(dfnew, paste0(path, "COH_Address_Locations.rds"))

########   rebuild dfnew from scratch. Now it only needs to drive the 
########   averaging process. Update dfnew accordingly and there will be no 
########   duplicates left

dfnew <- 
dfnew %>% 
  group_by(Zipcode, Street_name, Street_num, Street_type, Prefix) %>% 
  summarise(n=n(),
            Street_num=last(Street_num),
            Prefix=last(Prefix),
            Street_name=last(Street_name),
            Street_type=last(Street_type),
            City=last(CITY),
            Zipcode=last(Zipcode),
            Lat=mean(Y),
            Lon=mean(X),
            Xstd=sd(X_COORD),
            Ystd=sd(Y_COORD),
            X_COORD=mean(X_COORD),
            Y_COORD=mean(Y_COORD),
            OBJECTID=last(OBJECTID)
            ) %>% 
  select(Street_num, Prefix, Street_name, Street_type, City, Zipcode,
         n, Lat, Lon, Xstd, Ystd, X=X_COORD, Y=Y_COORD, OBJECTID)  


#   File suitable for using in geocoding

saveRDS(dfnew, paste0(path, "COH_Geocoding_Locations.rds"))

dt <- data.table::as.data.table(dfnew)
saveRDS(dt,paste0(path, "COH_Geocoding_Locations_table.rds"))

```
